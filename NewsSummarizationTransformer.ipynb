{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NewsSummarizationTransformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zabir173/Inshort-News-summarization-Transformer/blob/main/NewsSummarizationTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSLKu1OKQi3j"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as krs\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "import csv\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 50 # first it was 200"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsSE13bjQi3l"
      },
      "source": [
        "### Reading the data and droppping useless columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "uY0j9Ci2Qi3m",
        "outputId": "3d159e1b-2b4c-437e-8ca1-ed8960906dfb"
      },
      "source": [
        "\n",
        "data_unprocessed_news = pd.read_excel('Inshorts Cleaned Data.xlsx')\n",
        "data_unprocessed_news.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "      <td>The New Indian Express</td>\n",
              "      <td>09:25:00</td>\n",
              "      <td>2017-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "      <td>Outlook</td>\n",
              "      <td>22:18:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>23:39:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>23:08:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>23:24:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ... Publish Date\n",
              "0  4 ex-bank officials booked for cheating bank o...  ...   2017-03-26\n",
              "1     Supreme Court to go paperless in 6 months: CJI  ...   2017-03-25\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  ...   2017-03-25\n",
              "3  Why has Reliance been barred from trading in f...  ...   2017-03-25\n",
              "4  Was stopped from entering my own studio at Tim...  ...   2017-03-25\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "RpNwrRC_Qi3n",
        "outputId": "b342b735-8f25-430f-d635-c7e213a82daf"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "# shuffling the data \n",
        "data_unprocessed_news = shuffle(data_unprocessed_news)\n",
        "data_unprocessed_news.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7564</th>\n",
              "      <td>India eases visa policy for Afghanistan</td>\n",
              "      <td>India has further liberalised its visa policy ...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>12:06:00</td>\n",
              "      <td>2017-02-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>Jadeja scalps two after his quick 54* as Aus e...</td>\n",
              "      <td>All-rounder Ravindra Jadeja followed up his 54...</td>\n",
              "      <td>Cricket Country</td>\n",
              "      <td>17:17:00</td>\n",
              "      <td>2017-03-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29913</th>\n",
              "      <td>Longest-lasting lightning bolt struck France i...</td>\n",
              "      <td>A lightning strike in France on August 30, 201...</td>\n",
              "      <td>Phys</td>\n",
              "      <td>03:00:00</td>\n",
              "      <td>2016-09-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19842</th>\n",
              "      <td>Never went out on a date with Taylor Swift: Ed...</td>\n",
              "      <td>Hollywood actor Eddie Redmayne has revealed th...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>19:56:00</td>\n",
              "      <td>2016-11-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18238</th>\n",
              "      <td>Plans approved for London&amp;#39;s second-tallest...</td>\n",
              "      <td>Authorities have approved plans for a 73-store...</td>\n",
              "      <td>The Guardian</td>\n",
              "      <td>10:26:00</td>\n",
              "      <td>2016-11-29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Headline  ... Publish Date\n",
              "7564             India eases visa policy for Afghanistan  ...   2017-02-04\n",
              "1065   Jadeja scalps two after his quick 54* as Aus e...  ...   2017-03-19\n",
              "29913  Longest-lasting lightning bolt struck France i...  ...   2016-09-18\n",
              "19842  Never went out on a date with Taylor Swift: Ed...  ...   2016-11-18\n",
              "18238  Plans approved for London&#39;s second-tallest...  ...   2016-11-29\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "WCqfzAViQi3o",
        "outputId": "347f5732-d8ec-4bbd-f6e2-b926436284eb"
      },
      "source": [
        "summaries, longreview = pd.DataFrame(), pd.DataFrame()\n",
        "summaries['short'] = data_unprocessed_news['short']#[:data_to_use]\n",
        "longreview['long'] = data_unprocessed_news['long']#[:data_to_use]\n",
        "(summaries.shape,longreview.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'short'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5fba20d7163b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_unprocessed_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#[:data_to_use]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlongreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_unprocessed_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'long'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#[:data_to_use]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlongreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'short'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6dHF3wMQi3p"
      },
      "source": [
        "### Cleaning the data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFrlz62jQi3q"
      },
      "source": [
        "# replacing many abbreviations and lower casing the words\n",
        "def clean_words(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') # for converting é to e and other accented chars\n",
        "    sentence = re.sub(r\"http\\S+\",\"\",sentence)\n",
        "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "    sentence = re.sub(r\"'til\", \"until\", sentence)\n",
        "    sentence = re.sub(r\"\\\"\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\'\", \"\", sentence)\n",
        "    sentence = re.sub(r' s ', \"\",sentence)\n",
        "    sentence = re.sub(r\"&39\", \"\", sentence) # the inshorts data has this in it\n",
        "    sentence = re.sub(r\"&34\", \"\", sentence) # the inshorts data has this in it\n",
        "    sentence = re.sub(r\"[\\[\\]\\\\0-9()\\\"$#%/@;:<>{}`+=~|.!?,-]\", \"\", sentence)\n",
        "    sentence = re.sub(r\"&\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\\\n\", \"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n",
        "    return text\n",
        "    \n",
        "article = article.apply(lambda x: preprocess(x))\n",
        "summary = summary.apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFlBoLsrQi3r"
      },
      "source": [
        "summaries['short'] = summaries['short'].apply(lambda x: clean_words(x))\n",
        "longreview['long'] = longreview['long'].apply(lambda x: clean_words(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSVH7yD4Qi3r"
      },
      "source": [
        "longreview.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE_Dz408Qi3s"
      },
      "source": [
        "# adding start and end token to the senteces of label \n",
        "start_token, end_token = '<startseq>' , '<endseq>'\n",
        "summaries = summaries.apply(lambda x: start_token + ' ' + x + ' ' + end_token)\n",
        "summaries.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBMM3ZZfQi3s"
      },
      "source": [
        "val_split = 0.1\n",
        "# train validation split\n",
        "summaries_train = summaries[int(len(summaries)*val_split):]\n",
        "summaries_val = summaries[:int(len(summaries)*val_split)]\n",
        "longreview_train = longreview[int(len(summaries)*val_split):]\n",
        "longreview_val = longreview[:int(len(summaries)*val_split)]\n",
        "\n",
        "len(longreview_val),len(longreview_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktHiFdZnQi3t"
      },
      "source": [
        "longreview_train.iloc[0], summaries_train.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PYpVuIlQi3t"
      },
      "source": [
        "finding the maximum length of questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZH1S0xSQi3u"
      },
      "source": [
        "# because there are senteces with unusually long lengths, \n",
        "# we caculate the max length that 95% of sentences are shorter than that\n",
        "def max_length(shorts, longs, prct):\n",
        "    # Create a list of all the captions\n",
        "    \n",
        "    length_longs = list(len(d.split()) for d in longs)\n",
        "    length_shorts = list(len(d.split()) for d in shorts)\n",
        "\n",
        "    print('percentile {} of length of news: {}'.format(prct,np.percentile(length_longs, prct)))\n",
        "    print('longest sentence: ', max(length_longs))\n",
        "    print()\n",
        "    print('percentile {} of length of summaries: {}'.format(prct,np.percentile(length_shorts, prct)))\n",
        "    print('longest sentence: ', max(length_shorts))\n",
        "    print()\n",
        "    return int(np.percentile(length_longs, prct)),int(np.percentile(length_shorts, prct))\n",
        "\n",
        "# selecting sentence length based on the percentile of data that fits in the length\n",
        "max_len_news, max_len_summary= max_length(summaries_train['short'].to_list(), longreview_train['long'].to_list(), 90)\n",
        "\n",
        "\n",
        "print('max-length longreview chosen for training: ', max_len_news)\n",
        "print('max-length summaries chosen for training: ', max_len_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBEhriOQi3v"
      },
      "source": [
        "### Dataset prepration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTujzEhkQi3v"
      },
      "source": [
        "# making a vocabulary of the words \n",
        "def create_vocab(shorts, longs = None, minimum_repeat = 3):\n",
        "\n",
        "    # Create a list of all the captions\n",
        "    all_captions = []\n",
        "    for s in shorts:\n",
        "        all_captions.append(s)\n",
        "\n",
        "    # Consider only words which occur at least minimum_occurrence times in the corpus\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= minimum_repeat]\n",
        "    \n",
        "    vocab = list(set(vocab))\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O35lRQSVQi3w"
      },
      "source": [
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\n",
        "vocab_dec = create_vocab(summaries_train['short'].to_list(), minimum_repeat=5) # here we just use the words in vocabulary of summaries\n",
        "# removing one character words from vocab except for 'a'\n",
        "for v in vocab_dec:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_dec.remove(v) \n",
        "        \n",
        "vocab_dec = sorted(vocab_dec)[1:] # [1:] is for the '' \n",
        "vocab_dec[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZEqyBUIQi3w"
      },
      "source": [
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\n",
        "vocab_enc = create_vocab(longreview_train['long'].to_list(), minimum_repeat=3) # here we just use the words in vocabulary of summaries\n",
        "# removing one character words from vocab except for 'a'\n",
        "for v in vocab_enc:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_enc.remove(v) \n",
        "        \n",
        "vocab_enc = sorted(vocab_enc)[1:] # [1:] is for the '' \n",
        "vocab_enc[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWWuSDZTQi3x"
      },
      "source": [
        "oov_token = '<UNK>'\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' # making sure all the last non digit non alphabet chars are removed\n",
        "document_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "summary_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "document_tokenizer.fit_on_texts(vocab_enc)\n",
        "summary_tokenizer.fit_on_texts(vocab_dec)#summaries_train['short'])\n",
        "\n",
        "# caculating number of words in vocabulary of encoder and decoder\n",
        "# they are important for positional encoding\n",
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1 \n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3j_s_OUQi3y"
      },
      "source": [
        "ixtoword_enc = {} # index to word dic\n",
        "ixtoword_dec = {} # index to word dic\n",
        "\n",
        "wordtoix_enc = document_tokenizer.word_index # word to index dic\n",
        "ixtoword_enc[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\n",
        "ixtoword_dec[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\n",
        "\n",
        "for w in document_tokenizer.word_index:\n",
        "    ixtoword_enc[document_tokenizer.word_index[w]] = w\n",
        "################################################\n",
        "wordtoix_dec = summary_tokenizer.word_index # word to index dic\n",
        "\n",
        "for w in summary_tokenizer.word_index:\n",
        "    ixtoword_dec[summary_tokenizer.word_index[w]] = w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BShXAmpdQi3y"
      },
      "source": [
        "# assign a number to each word inorder to find it in word embeddings\n",
        "inputs = document_tokenizer.texts_to_sequences(longreview_train['long'])\n",
        "targets = summary_tokenizer.texts_to_sequences(summaries_train['short'])\n",
        "inputs_val = document_tokenizer.texts_to_sequences(longreview_val['long'])\n",
        "targets_val = summary_tokenizer.texts_to_sequences(summaries_val['short'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSpFkadDQi3y"
      },
      "source": [
        "inputs = krs.preprocessing.sequence.pad_sequences(inputs, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets = krs.preprocessing.sequence.pad_sequences(targets, maxlen=max_len_summary, padding='post', truncating='post')\n",
        "inputs_val = krs.preprocessing.sequence.pad_sequences(inputs_val, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets_val = krs.preprocessing.sequence.pad_sequences(targets_val, maxlen=max_len_summary, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_YrEW60Qi3z"
      },
      "source": [
        "# validate train split\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((inputs_val,targets_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE*2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0OZLBUBQi3z"
      },
      "source": [
        "longreview_val.reset_index(inplace=True, drop=True)\n",
        "summaries_val.reset_index(inplace=True, drop=True)\n",
        "longreview_train.reset_index(inplace=True, drop=True)\n",
        "summaries_train.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AStdZ9F1Qi3z"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CreD6JW0Qi30"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hist(history):\n",
        "    plt.title('Loss')\n",
        "\n",
        "    x= [i[0] for i in history['val']]\n",
        "    y=[i[1] for i in history['val']]\n",
        "    plt.plot(x,y,'x-')\n",
        "    \n",
        "    x= [i[0] for i in history['train']]\n",
        "    y=[i[1] for i in history['train']]    \n",
        "    plt.plot(x,y,'o-')\n",
        "\n",
        "    plt.legend(['validation','train'])\n",
        "    plt.show()\n",
        "    print('smallest val loss:', sorted(history['val'],key=lambda x: x[1])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYBbosixQi30"
      },
      "source": [
        "#### Scaled Dot Product\n",
        "![](files/z-score.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jugx4OQ_Qi30"
      },
      "source": [
        "# the job of this function is to calculate the above equation\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsvwSPzQi31"
      },
      "source": [
        "#### multi-headed attention\n",
        "![](files/multi-head.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Xm__qgQi31"
      },
      "source": [
        "class MultiHeadAttention(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads # The dimensions of Q, K, V are called depth\n",
        "\n",
        "        # the input of these 3 layers are the same: X\n",
        "        self.wq = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        self.wk = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        self.wv = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "    \n",
        "    # reshape the Q,K,V \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        # learn the Q,K,V matrices (the layers' weightes)\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        \n",
        "        # reshape them\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        # the last dens layer expect one vector so we use concat\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtcD0cfUQi32"
      },
      "source": [
        "#### positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpksLICjQi32"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates\n",
        "\n",
        "# The dimension of positional encodings is the same as\n",
        "# the embeddings (d_model) for facilitating the summation of both.\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKFXCpmMQi33"
      },
      "source": [
        "#### Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKm28N_DQi33"
      },
      "source": [
        "#### Embeddings preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRP8_xEaQi33"
      },
      "source": [
        " # Making the embedding mtrix\n",
        "def make_embedding_layer(vocab_len, wordtoix, embedding_dim=200, glove=True, glove_path= '../glove'):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
        "    else:\n",
        "        print('Loading glove...')\n",
        "        glove_dir = glove_path\n",
        "        embeddings_index = {} \n",
        "        f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        # Get n-dim dense vector for each of the vocab_rocc\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "        print(\"GloVe \",embedding_dim, ' loaded!')\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
        "                                                                                           # do not train the embedding layer\n",
        "                                                                                           # we use 0 as padding so => mask_zero=True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    return embedding_layer\n",
        "\n",
        "embeddings_encoder = make_embedding_layer(encoder_vocab_size, wordtoix_enc, embedding_dim=embedding_dim, glove=True)\n",
        "embeddings_decoder = make_embedding_layer(decoder_vocab_size, wordtoix_dec, embedding_dim=embedding_dim, glove=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUyNcIWQi34"
      },
      "source": [
        "#### transformer layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "475-EmqeQi34"
      },
      "source": [
        "# hyper-params\n",
        "init_lr = 1e-3\n",
        "lmbda_l2 = 0.1\n",
        "d_out_rate = 0.1 # tested 0.4, 0.3, 0.1 values this 0.1 seems to be the best\n",
        "num_layers = 4 # chaged from 4 to 5 to learn better\n",
        "d_model = embedding_dim # d_model is the representation dimension or embedding dimension of a word (usually in the range 128–512)\n",
        "dff = 512 # number of neurons in feed forward network\n",
        "num_heads = 5 # first it was 8 i chenged it to 10 to use embd =300d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMFvf_-JQi35"
      },
      "source": [
        "# The Point-wise feed-forward network block is essentially a \n",
        "# two-layer linear transformation which is used identically throughout the model\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return krs.Sequential([\n",
        "        krs.layers.Dense(dff, activation='relu',kernel_regularizer=krs.regularizers.l2(l=lmbda_l2)),\n",
        "        krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "47HEMjjqQi35"
      },
      "source": [
        "class EncoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "   \n",
        "    # it has 1 layer of multi-headed attention\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6vn1H-jQi35"
      },
      "source": [
        "class DecoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "        self.dropout3 = krs.layers.Dropout(rate)\n",
        "    \n",
        "    # it has 2 layers of multi-headed attention\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjU9loNPQi36"
      },
      "source": [
        "class Encoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_encoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "        self.dropout_embd = krs.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embd(x, training=training) # dropout added to encoder input changed from nothing to this\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7MUsxIEQi37"
      },
      "source": [
        "class Decoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_decoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] # a list of decoder layers\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask) # enc_output is fed into it\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDtGTJuTQi37"
      },
      "source": [
        "#### Final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHCfZt9UQi37"
      },
      "source": [
        "class Transformer(krs.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                                     target_vocab_size, pe_input, pe_target, rate=d_out_rate):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = krs.layers.Dense(target_vocab_size, kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        \n",
        "        \n",
        "    # training argument is used in dropout inputs\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "       \n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DME7c0LAQi38"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=max_len_news,\n",
        "    pe_target=max_len_summary,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxcq-2V0Qi38"
      },
      "source": [
        "#### Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuJ9YF2UQi38"
      },
      "source": [
        "# Padding mask for masking \"pad\" sequences so \n",
        "# they won't affect the loss\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# Lookahead mask for masking future words from\n",
        "# contributing in prediction of current words in self attention\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-vmHE-Qi39"
      },
      "source": [
        "# this function is use in training step\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "        \n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8k7tf1Qi39"
      },
      "source": [
        "#### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkaw205mQi39"
      },
      "source": [
        "lr_schedule = krs.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=init_lr, # originally was 1e-5\n",
        "    decay_steps=4000, # approximately 5 epochs\n",
        "    decay_rate=0.95) # originally was 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEKORlX8Qi39"
      },
      "source": [
        "\n",
        "optimizer2 = Adam(lr_schedule , beta_1=0.9, beta_2=0.98, epsilon=1e-9) # changed to init\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # added softmax changed from_logits to false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GaSdY6_Qi3-"
      },
      "source": [
        "def loss_function(real, pred, l2= False):\n",
        " \n",
        "    if l2:\n",
        "        lambda_ = 0.0001\n",
        "        l2_norms = [tf.nn.l2_loss(v) for v in transformer.trainable_variables]\n",
        "        l2_norm = tf.reduce_sum(l2_norms)\n",
        "        l2_value = lambda_ * l2_norm\n",
        "        loss_ = loss_object(real, pred) + l2_value\n",
        "    else:\n",
        "        loss_ = loss_object(real, pred) \n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-cikYpJQi3-"
      },
      "source": [
        "checkpoint_path4 =\"checkpoints4\"\n",
        "\n",
        "ckpt4 = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer2)\n",
        "\n",
        "ckpt_manager4 = tf.train.CheckpointManager(ckpt4, checkpoint_path4, max_to_keep=100)\n",
        "\n",
        "# if ckpt_manager4.latest_checkpoint:\n",
        "#     ckpt4.restore(ckpt_manager4.latest_checkpoint)\n",
        "#     print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Hm4B5uQi3-"
      },
      "source": [
        "#### inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Unbo8vQi3-"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = krs.preprocessing.sequence.pad_sequences(input_document, maxlen=max_len_news, \n",
        "                                                                           padding='post', truncating='post')\n",
        "    \n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[start_token]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_len_summary):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # stop prediciting if it reached end_token\n",
        "        if predicted_id == summary_tokenizer.word_index[end_token]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # remove start_token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "85u_7szYQi3_"
      },
      "source": [
        "def validate():\n",
        "    print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] # <startseq> hi im moein\n",
        "        tar_real = tar[:, 1:] # hi im moein <endseq>\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "        # Operations are recorded if they are executed within this context manager\n",
        "        # and at least one of their inputs is being \"watched\". Trainable variables are automatically watched\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            False, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    return val_loss.result()\n",
        "# validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cBja89fQi3_"
      },
      "source": [
        "@tf.function # Compiles a function into a callable TensorFlow graph\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1] # <startseq> hi im moein\n",
        "    tar_real = tar[:, 1:] # hi im moein <endseq>\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    # Operations are recorded if they are executed within this context manager\n",
        "    # and at least one of their inputs is being \"watched\". Trainable variables are automatically watched\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer2.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    # mean the loss with new computed  loss of the step\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkz8j49OQi4A"
      },
      "source": [
        "history={'val':[],'train':[]}\n",
        "EPOCHS = 300\n",
        "not_progressing = 0\n",
        "# Computes the (weighted) mean of the given loss values.\n",
        "train_loss = krs.metrics.Mean(name='train_loss')\n",
        "val_loss = krs.metrics.Mean(name='val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sngBbCTJQi4A"
      },
      "source": [
        "params = {\n",
        "'lmbda_l2' : lmbda_l2,\n",
        "'d_out_rate' :d_out_rate,\n",
        "'num_layers' : num_layers ,\n",
        "'d_model' : d_model  ,\n",
        "'dff' : dff ,\n",
        "'num_heads' : num_heads,\n",
        "'init_lr':init_lr}\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i9hj7AVQi4A"
      },
      "source": [
        "ep = 1\n",
        "best_val_loss = np.inf\n",
        "i1,i2,i3,i4 = np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YAe_j4JUQi4B"
      },
      "source": [
        "print(params)\n",
        "print('#'*40)\n",
        "\n",
        "for epoch in range(ep,EPOCHS+1):\n",
        "    ep = epoch\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        \n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 150 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch , batch, train_loss.result()))\n",
        "                  \n",
        "    print()\n",
        "    print(summarize(clean_words(longreview_val['long'][i1])))\n",
        "    print(summarize(clean_words(longreview_val['long'][i2])))\n",
        "    print(summarize(clean_words(longreview_val['long'][i3])))\n",
        "    print(summarize(clean_words(longreview_val['long'][i4])))\n",
        "    print()\n",
        "    \n",
        "    val_loss_ = validate().numpy()\n",
        "    history['val'].append((epoch,val_loss_))\n",
        "    print ('\\n* Train Loss {:.4f}'.format(train_loss.result()))\n",
        "    history['train'].append((epoch,train_loss.result().numpy()))\n",
        "    \n",
        "    \n",
        "    if best_val_loss-val_loss_ > 0.1:\n",
        "        ckpt_save_path4 = ckpt_manager4.save()\n",
        "        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path4))  \n",
        "        best_val_loss = val_loss_\n",
        "    \n",
        "    hist(history)\n",
        "    print('Current Lr: ',optimizer2._decayed_lr('float32').numpy())\n",
        "    print ('\\nTime taken for this epoch: {:.2f} secs\\n'.format(time.time() - start))\n",
        "    print('='*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksH8inVDQi4B"
      },
      "source": [
        "output removed to reduce file size for github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjJuDZpQi4B"
      },
      "source": [
        "hist(history)\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uzzAfvSQi4C"
      },
      "source": [
        "you can use more data or more regularization to avoid the overfitting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6A_BHilQi4C"
      },
      "source": [
        "print(clean_words(longreview_val['long'][i1]))\n",
        "print()\n",
        "print(summarize(clean_words(longreview_val['long'][i1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdGmQTOoQi4C"
      },
      "source": [
        "print(clean_words(longreview_val['long'][i2]))\n",
        "print()\n",
        "print(summarize(clean_words(longreview_val['long'][i2])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6FGzp_Qi4D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}